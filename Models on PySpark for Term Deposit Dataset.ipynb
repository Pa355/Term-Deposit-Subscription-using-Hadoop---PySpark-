{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data file \n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "spark_df = sqlContext.sql(\"Select * from TermDeposit_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the missing values\n",
    "null_count_list = []\n",
    "for i in spark_df.columns:\n",
    "null_count = spark_df.select(i).where(col(i).isNull()).count()\n",
    "null_count_list.append([i,null_count])\n",
    "null_count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyze churn distribution\n",
    "\n",
    "import pandas as pd\n",
    "#pandas_df = \n",
    "spark_df.select('deposit').groupBy('deposit').count().show()\n",
    "#pandas_df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize all variables \n",
    "\n",
    "summary = spark_df.describe().toPandas().transpose()\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define target variable and segregate categorical and numerical variables\n",
    "\n",
    "target = 'deposit'\n",
    "dtypes = spark_df.dtypes\n",
    "cat_input = []\n",
    "for i in range(0, len(spark_df.columns)):\n",
    "if dtypes[i][1] == 'string':\n",
    "    cat_input.append(dtypes[i][0])\n",
    "cat_input = list(set(cat_input)-set(target))\n",
    "cat_input\n",
    "\n",
    "num_input = list(set(spark_df.columns) - set([target]) - set(cat_input))\n",
    "num_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the  correlations\n",
    "import matplotlib.pyplot as plt\n",
    "numeric_data = spark_df.select(num_input).toPandas()\n",
    "axs = pd.scatter_matrix(numeric_data, figsize=(8, 8));\n",
    "n = len(numeric_data.columns)\n",
    "for i in range(n):\n",
    "    v = axs[i, 0]\n",
    "    v.yaxis.label.set_rotation(45)\n",
    "    v.yaxis.label.set_ha('right')\n",
    "    v.set_yticks(())\n",
    "    h = axs[n-1, i]\n",
    "    h.xaxis.label.set_rotation(45)\n",
    "    h.set_xticks(())\n",
    "display(plt.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualize boxplot for numerical variables\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt1\n",
    "\n",
    "numeric_data1 = spark_df.select(num_input).toPandas()\n",
    "numeric_data1.plot(kind='box', subplots=True, layout=(2,4), sharex=False, sharey=False, figsize=(15,7))\n",
    "display(plt1.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vizualize histogram for numerical variables\n",
    "\n",
    "import pylab as plt\n",
    "numeric_data1.hist(bins=30, figsize=(14,8))\n",
    "pl.suptitle(\"Histogram for each numeric input variable\")\n",
    "#plt.savefig('fruits_hist')\n",
    "display(plt.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize categorical variables\n",
    "\n",
    "#Relationship between marital by deposit (Y/N) \n",
    "#notice the difference in marital between the 2 classes (graphs displayed in the following 2 blocks)\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# show marital by deposit=yes table\n",
    "marital_data_yes = spark_df.where(\"deposit = 'yes'\").groupBy('marital').agg(F.count('marital').alias('count_yes'))\n",
    "total = marital_data_yes.select(\"count_yes\").agg(F.sum('count_yes').alias('total')).collect().pop()['total']\n",
    "marital_data_yes = marital_data_yes.withColumn('ratio', (marital_data_yes['count_yes']/total)*100)\n",
    "marital_data_yes.show()\n",
    "\n",
    "# show marital by deposit=no table\n",
    "marital_data_no = spark_df.where(\"deposit = 'no'\").groupBy('marital').agg(F.count('marital').alias('count_no'))\n",
    "total = marital_data_no.select(\"count_no\").agg(F.sum('count_no').alias('total')).collect().pop()['total']\n",
    "marital_data_no = marital_data_no.withColumn('ratio', (marital_data_no['count_no']/total)*100)\n",
    "marital_data_no.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot marital by deposit=yes\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.figure(figsize=(4,3))\n",
    "pandas_df = marital_data_yes.toPandas()\n",
    "pandas_df.sort_values('ratio', axis = 0, ascending=False, inplace=True)\n",
    "marital = pandas_df['marital']\n",
    "ratio = pandas_df['ratio']\n",
    "x = np.arange(len(marital))\n",
    "plt.bar(x, ratio)\n",
    "plt.xticks(x, marital, rotation='horizontal')\n",
    "plt.title('Deposit = Yes')\n",
    "display(plt.show())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot marital by deposit=no\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.figure(figsize=(4,3))\n",
    "pandas_df = marital_data_no.toPandas()\n",
    "pandas_df.sort_values('ratio', axis = 0, ascending=False, inplace=True)\n",
    "marital = pandas_df['marital']\n",
    "ratio = pandas_df['ratio']\n",
    "x = np.arange(len(marital))\n",
    "plt.bar(x, ratio)\n",
    "plt.xticks(x, marital, rotation='horizontal')\n",
    "plt.title('Deposit = No')\n",
    "display(plt.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the columns\n",
    "\n",
    "df = spark_df.select('age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan', 'contact', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'deposit')\n",
    "cols = df.columns\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using transformers StringIndexer, encoder, assembler\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "categoricalColumns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome']\n",
    "stages = []\n",
    "for categoricalCol in categoricalColumns:\n",
    "\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol = 'deposit', outputCol = 'label')\n",
    "stages += [label_stringIdx]\n",
    "\n",
    "numericCols = ['age', 'balance', 'duration', 'campaign', 'pdays', 'previous']\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the pipeline \n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(df)\n",
    "df = pipelineModel.transform(df)\n",
    "selectedCols = ['label', 'features'] + cols\n",
    "df = df.select(selectedCols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df.take(5), columns=df.columns).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into trainimg and testing (70/30)\n",
    "\n",
    "train, test = df.randomSplit([0.7, 0.3], seed = 2018)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run logistic regression on training data\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions on the test data\n",
    "\n",
    "predictions = lrModel.transform(test)\n",
    "predictions.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print('Test Area Under ROC', evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 1 - DecisionTree classifier\n",
    "\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)\n",
    "dtModel = dt.fit(train)\n",
    "predictions = dtModel.transform(test)\n",
    "predictions.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)\n",
    "\n",
    "#evaluate performance\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 2 - RandomForest classifier\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label')\n",
    "rfModel = rf.fit(train)\n",
    "predictions = rfModel.transform(test)\n",
    "predictions.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)\n",
    "\n",
    "#evaluate performance\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 3 - Gradient Boosted Tree\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(maxIter=10)\n",
    "gbtModel = gbt.fit(train)\n",
    "predictions = gbtModel.transform(test)\n",
    "predictions.select('age', 'job', 'label', 'rawPrediction', 'prediction', 'probability').show(10)\n",
    "\n",
    "#evaluate performance\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix \n",
    "\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix,roc_curve, auc\n",
    "from sklearn import svm, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y = np.array(predictions.toPandas()['label'])\n",
    "scores = np.array(predictions.toPandas()['prediction'])\n",
    "confusion_matrix(y,scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ROC curve plot \n",
    "\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# save the predicted and probabilities in y and scores variables\n",
    "y = predictions.select(\"label\").rdd.flatMap(lambda x: x).collect()\n",
    "scores = predictions.select(\"probability\").rdd.map(lambda x: x[0][1]).collect()\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y, scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(fpr)\n",
    "print(tpr)\n",
    "print(thresholds)\n",
    "print(roc_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC plot Curve vizualization\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.gcf().clear()\n",
    "plt.plot(fpr, tpr, color = 'Green', label='ROC curve (area = %0.3f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "display(plt.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicted Deciles\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "deciles = pd.qcut(scores, 10, labels=False)  # decile column\n",
    "\n",
    "pred_deciles = pd.DataFrame(\n",
    "    {'deposit': y,\n",
    "     'Probabilities': scores,\n",
    "     'Decile Number': deciles\n",
    "    })\n",
    "\n",
    "pred_deciles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decile table\n",
    "\n",
    "grouper = pred_deciles[['Decile Number', 'deposit']].groupby('Decile Number')\n",
    "decile_table = grouper.count()\n",
    "decile_table['Target'] = grouper.sum()['deposit']\n",
    "decile_table['Nontarget'] = decile_table['deposit'] - decile_table['Target']\n",
    "\n",
    "decile_table = decile_table.rename(index=str, columns={\"deposit\": \"Decile Size\"})\n",
    "decile_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross-validation\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(gbt.maxDepth, [2, 4, 6])\n",
    "             .addGrid(gbt.maxBins, [20, 60])\n",
    "             .addGrid(gbt.maxIter, [10, 20])\n",
    "             .build())\n",
    "cv = CrossValidator(estimator=gbt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "# Run cross validations.This can take about 16 minutes!\n",
    "cvModel = cv.fit(train)\n",
    "predictions = cvModel.transform(test)\n",
    "evaluator.evaluate(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "name": "Project 1_bank",
  "notebookId": 2657530868653812
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
